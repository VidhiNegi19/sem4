{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VidhiNegi19/sem4/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1**    -Vidhi Negi\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Car Price Prediction using Machine Learning**\n",
        "\n",
        "This project aimed to predict car prices based on various features using machine learning. The project involved:\n",
        "\n",
        "1. **Data Exploration and Preprocessing:** Understanding the dataset, handling missing values, encoding categorical variables, and scaling numerical features.\n",
        "\n",
        "2. **Exploratory Data Analysis (EDA):** Using visualizations to identify patterns and relationships between features and car prices.\n",
        "\n",
        "3. **Feature Engineering:** Creating new features (e.g., engine size/horsepower ratio, fuel efficiency) to enhance model accuracy.\n",
        "\n",
        "4. **Model Building and Evaluation:** Training and evaluating multiple machine learning models (e.g., Linear Regression, Ridge Regression, etc.) to find the best predictor.\n",
        "5.** Model Optimization:** Fine-tuning model hyperparameters and using cross-validation to improve performance.\n",
        "\n",
        "**Key Findings:**\n",
        "\n",
        "1. Strong correlations were observed between engine size, horsepower, and car price.\n",
        "2. Feature engineering improved prediction accuracy.\n",
        "3. The selected model achieved promising results in predicting car prices.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "This project provides a valuable tool for car dealerships, consumers, and manufacturers to estimate car prices, make informed purchase decisions, and optimize pricing strategies.\n",
        "\n",
        "**In Essence:**\n",
        "\n",
        "The project successfully built a machine learning model to predict car prices with reasonable accuracy, providing valuable insights for stakeholders in the automotive industry.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/VidhiNegi19/sem4"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The automotive market is dynamic and complex, with car prices influenced by a multitude of factors. Accurately predicting car prices is crucial for various stakeholders, including buyers, sellers, and manufacturers. This project addresses the challenge of developing a reliable and accurate model to predict car prices based on a comprehensive set of features."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "4_S1eKFtGOBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(7)"
      ],
      "metadata": {
        "id": "YyqPFM0eGWIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "UE3n52pwGV72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "JilCVZISGn0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:15,:-20]"
      ],
      "metadata": {
        "id": "56y8wBxVHP--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:100,['symboling','CarName']]"
      ],
      "metadata": {
        "id": "VTXAXmYFHgeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[20:50]"
      ],
      "metadata": {
        "id": "FBNLIBJNH-A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_reviewdf = pd.read_csv('/content/CarPrice_project (3).csv')\n"
      ],
      "metadata": {
        "id": "5o64y-dkKCot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "car_review_df = car_reviewdf.copy()\n"
      ],
      "metadata": {
        "id": "1cNOuG3TKW0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "car_review_df.head()"
      ],
      "metadata": {
        "id": "s8p65qFTKbfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count\n"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'number of rows:{car_review_df.shape[0]}') # Changed 'caprintr_review_df' to 'car_review_df'\n",
        "print(f'number of columns:{car_review_df.shape[1]}')"
      ],
      "metadata": {
        "id": "qRGmPd4gJ7Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def complete_info():\n",
        "  null = pd.DataFrame(index=car_review_df.columns)\n",
        "  null['data_type'] = car_review_df.dtypes\n",
        "  null['null_count'] = car_review_df.isnull().sum()\n",
        "  null['unique_count'] = car_review_df.nunique()\n",
        "  return null"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_info()"
      ],
      "metadata": {
        "id": "s0BYuKeTK9VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "car_review_df.drop_duplicates(inplace=True )"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Valuees/Null Values Count\n",
        "car_review_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(car_review_df.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer *Here*\n",
        "\n",
        "\n",
        "The dataset seems to be relatively clean with no missing values or duplicates. The provided features could be used for potential analysis or prediction tasks related to car prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "car_review_df['CarName']\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "car_review_df.describe()\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Understanding the data types and descriptions of your variables is crucial for selecting appropriate analysis techniques and interpreting the results."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in car_review_df.columns:\n",
        "  print(f\"unique values for {column}:{car_review_df[column].unique()}\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'car_reviewdf' is the original DataFrame containing 'CarName'\n",
        "car_review_df = car_reviewdf.copy()  # Create a copy to avoid modifying the original\n",
        "\n",
        "car_review_df['CarCompany'] = car_review_df['CarName'].apply(lambda x: x.split(' ')[0])\n",
        "car_review_df.drop('CarName', axis=1, inplace=True)\n",
        "\n",
        "car_review_df['CarCompany'] = car_review_df['CarCompany'].replace({'maxda': 'mazda', 'porcshce': 'porsche', 'toyouta': 'toyota', 'vokswagen': 'volkswagen', 'vw': 'volkswagen'})\n",
        "\n",
        "categorical_features = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'CarCompany']\n",
        "encoded_df = pd.get_dummies(car_review_df, columns=categorical_features, drop_first=True) # drop_first avoids multicollinearity\n",
        "\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "By performing these manipulations, the data is cleaned, transformed, and prepared for further analysis and modeling. The insights gained from data wrangling can guide subsequent steps in the data science process and contribute to building more accurate and effective predictive models."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "sns.histplot(car_review_df['price'], bins=20)  # Adjust bins as needed\n",
        "plt.title('Distribution of Car Prices')\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A histogram was chosen because it is a suitable chart type for visualizing the distribution of a single numerical variable, which is the 'price' variable in this case. It effectively shows the frequency of cars at different price ranges, enabling us to understand the overall price distribution and identify potential patterns. While other chart types might be useful for different purposes, the histogram is the most appropriate choice for this specific visualization task. I hope this helps clarify the choice of chart for visualization.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The specific insights and their interpretation will depend on the actual shape and characteristics of the histogram generated from your data.\n",
        "\n",
        "By carefully analyzing the histogram, you can gain valuable insights into the distribution of car prices, which can be useful for further analysis, modeling, and decision-making. I hope this helps you understand the insights that can be found from the chart."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "By carefully considering the insights gained from the car price distribution histogram and taking appropriate actions, businesses can leverage these insights to create a positive business impact and avoid potential risks for negative growth. I hope this is useful"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# ploting distribution graph for Price\n",
        "plt.figure(figsize=(14,5))\n",
        "sns.distplot(car_review_df['price'],color = 'blue')\n",
        "plt.grid()\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "sns.distplot() was chosen because it provides a comprehensive and visually appealing representation of the car price distribution, including both frequency and density information. It is well-suited for continuous data and helps in identifying patterns, outliers, and the overall shape of the distribution. While other chart types might be useful for different purposes, sns.distplot() is a strong choice for this specific visualization task.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "By carefully analyzing the distribution plot, you can gain valuable insights into the distribution of car prices, which can be useful for further analysis, modeling, and decision-making."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "By carefully considering the insights gained from the car price distribution plot and taking appropriate actions, businesses can leverage these insights to create a positive business impact and avoid potential risks for negative growth."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "# Assuming 'car_reviewdf' is the original DataFrame containing 'CarName'\n",
        "# Load the dataset if it hasn't been loaded already\n",
        "# df = pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "# car_review_df = df.copy() # Ensure car_review_df is a copy of the loaded data\n",
        "\n",
        "# --- Data Wrangling Code (Ensure this runs before plotting) ---\n",
        "# Assuming 'car_review_df' exists from previous steps\n",
        "car_review_df['CarCompany'] = car_review_df['CarName'].apply(lambda x: x.split(' ')[0])\n",
        "car_review_df.drop('CarName', axis=1, inplace=True)\n",
        "\n",
        "car_review_df['CarCompany'] = car_review_df['CarCompany'].replace({'maxda': 'mazda', 'porcshce': 'porsche', 'toyouta': 'toyota', 'vokswagen': 'volkswagen', 'vw': 'volkswagen'})\n",
        "\n",
        "# Keep the rest of your data wrangling steps if needed, but they aren't directly related to this error\n",
        "# categorical_features = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'CarCompany']\n",
        "# encoded_df = pd.get_dummies(car_review_df, columns=categorical_features, drop_first=True) # drop_first avoids multicollinearity\n",
        "# X = encoded_df.drop('price', axis=1)\n",
        "# y = encoded_df['price']\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# Chart - 3 visualization code\n",
        "# Assuming 'car_review_df' has now been processed with the 'CarCompany' column added\n",
        "\n",
        "# Re-create top_10_cars *after* the 'CarCompany' column is created in car_review_df\n",
        "top_10_cars = car_review_df.head(10) # Now car_review_df should have 'CarCompany'\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(top_10_cars['CarCompany'], top_10_cars[\"citympg\"], marker='o', linestyle='-', color='b')\n",
        "\n",
        "plt.xlabel('CarCompany', fontsize=12)\n",
        "plt.ylabel(\"City Mileage (MPG)\", fontsize=12)\n",
        "plt.title(\"Top 10 Cars vs City Mileage\", fontsize=15)\n",
        "\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "a line plot was chosen for this visualization because it effectively shows the trend of city mileage across different car companies within the top 10 cars in your dataset. Its ability to highlight trends, compare values, and represent continuous data makes it a suitable choice for this specific scenario. Other chart types might be considered if the data or the objective of the visualization were different, but in this case, a line plot is the most appropriate option."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "hese insights are based on general observations from a line plot visualizing city mileage against car companies. The specific insights you can derive will depend on the actual data in your 'top_10_cars' DataFrame. Analyzing the shape of the line, the position of data points, and any noticeable patterns will provide you with more detailed insights relevant to your specific dataset."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "the insights gained from the city mileage line plot can have a significant positive impact on businesses by enabling targeted marketing, product development, inventory management, and pricing strategies. However, it's essential to consider the insights in conjunction with other factors and avoid overemphasizing or misinterpreting them to mitigate any potential negative growth implications. Continuous monitoring and adaptation to changing market trends are key for businesses to leverage the insights effectively and maintain a competitive edge."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Plotting a line graph to determine size distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.xlabel(\"Price\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid()\n",
        "size_distribution_graph = sns.kdeplot(car_review_df['price'], color=\"lightgreen\", shade = True)\n",
        "plt.title('Average price',size = 20);"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer this\n",
        "\n",
        "a KDE plot was chosen for this visualization because it's an effective way to visualize the distribution of car prices, which is continuous data. It provides a smooth, density-based representation that helps identify patterns, outliers, and the overall shape of the distribution. These insights can be valuable for understanding the price range of cars in the dataset and identifying potential trends or anomalies.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The specific insights you can derive will depend on the actual shape and characteristics of the KDE plot generated from your data.\n",
        "It's important to remember that KDE plots are estimates of the true probability density function, and there might be some variability in the representation depending on the kernel bandwidth used.\n",
        "Analyzing the KDE plot in conjunction with other visualizations and statistical measures can provide a more comprehensive understanding of the car price distribution."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "the insights gained from the KDE plot of car prices can have a significant positive impact on businesses by enabling informed pricing, inventory management, marketing, and product development strategies. However, it's important to interpret the insights carefully, consider outliers, and remain adaptable to changing market conditions to avoid any potential negative growth implications. A holistic approach that balances price considerations with other factors is crucial for achieving sustainable business success.\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "numerical_features = car_review_df.select_dtypes(include=np.number)\n",
        "correlation_matrix = numerical_features.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " a correlation heatmap was chosen for this visualization because it's an excellent tool for exploring relationships between multiple numerical variables, visualizing correlation coefficients, identifying patterns, and guiding data exploration and feature selection. It provides a concise and informative representation of the relationships within your car dataset, enabling you to gain valuable insights into the data.Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.scatter(car_review_df['enginesize'],car_review_df['price'],alpha=0.5)\n",
        "plt.title('enginesize vs price')\n",
        "plt.xlabel('enginesize')\n",
        "plt.ylabel('price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "You are likely using the scatter plot to explore the relationship between enginesize and price of cars in your dataset. By visualizing the data points, you can observe if there is a correlation between these two variables and potentially gain insights into how engine size influences car prices. This information can be valuable for further analysis, such as building a predictive model for car prices."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "You are likely using the scatter plot to explore the relationship between enginesize and price of cars in your dataset. By visualizing the data points, you can observe if there is a correlation between these two variables and potentially gain insights into how engine size influences car prices. This information can be valuable for further analysis, such as building a predictive model for car prices."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the scatter plot can be valuable for businesses, but they need to be interpreted and applied carefully in the context of broader market dynamics and customer preferences. Overemphasizing engine size as the sole determinant of price or neglecting other important factors could lead to negative growth implications. A holistic approach that balances engine size with other features, fuel efficiency, price sensitivity, and market trends is essential for achieving sustainable business success.\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.scatter(car_review_df['peakrpm'],car_review_df['price'],alpha=0.5)\n",
        "plt.title('peakrpm vs price')\n",
        "plt.xlabel('peakrpm')\n",
        "plt.ylabel('price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A scatter plot is used in this specific scenario because it is the most effective way to visually explore and understand the relationship between two numerical variables, enginesize and price, in your car dataset. It allows you to identify correlations, patterns, and outliers, providing valuable insights for business decisions related to pricing, product development, marketing, and inventory management."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "These insights are based on general observations from a scatter plot. The specific patterns and insights you derive will depend on the actual data in your car_review_df DataFrame. Carefully analyzing the shape, distribution, and clustering of points will provide you with more detailed and context-specific insights."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the scatter plot can be valuable for businesses, but they need to be interpreted and applied carefully in the context of broader market dynamics and customer preferences. Overemphasizing engine size as the sole determinant of price or neglecting other important factors could lead to negative growth implications. A holistic approach that balances engine size with other features, fuel efficiency, price sensitivity, and market trends is essential for achieving sustainable business success."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression # Assuming Linear Regression as an example\n",
        "import pandas as pd # Import pandas as it's used in data wrangling\n",
        "\n",
        "# --- Start of Data Wrangling Code (Copied from a previous cell) ---\n",
        "# Assuming 'car_reviewdf' is the original DataFrame containing 'CarName'\n",
        "# If car_reviewdf is not defined, you might need to load the data first.\n",
        "# For example:\n",
        "# car_reviewdf = pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "\n",
        "car_review_df = car_reviewdf.copy()  # Create a copy to avoid modifying the original\n",
        "\n",
        "car_review_df['CarCompany'] = car_review_df['CarName'].apply(lambda x: x.split(' ')[0])\n",
        "car_review_df.drop('CarName', axis=1, inplace=True)\n",
        "\n",
        "car_review_df['CarCompany'] = car_review_df['CarCompany'].replace({'maxda': 'mazda', 'porcshce': 'porsche', 'toyouta': 'toyota', 'vokswagen': 'volkswagen', 'vw': 'volkswagen'})\n",
        "\n",
        "categorical_features = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'CarCompany']\n",
        "encoded_df = pd.get_dummies(car_review_df, columns=categorical_features, drop_first=True) # drop_first avoids multicollinearity\n",
        "# --- End of Data Wrangling Code ---\n",
        "\n",
        "\n",
        "# Assuming 'X' and 'y' are your feature and target data\n",
        "# If not defined, replace with your actual feature and target data\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# Now you can calculate residuals\n",
        "residuals = Y_test - Y_pred\n",
        "\n",
        "# Visualize residuals\n",
        "plt.scatter(Y_pred, residuals)\n",
        "plt.title('Residuals vs. Predicted')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "In essence, the plot helps validate the model's assumptions and provides insight into the model's fit, potentially impacting the reliability of business decisions based on its predictions."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        " addressing insights from the residual plot can improve model accuracy and support positive business decisions, while ignoring them can hinder growth and profitability."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "def categorize_price(price):\n",
        "    if price > 50000:\n",
        "        return \"Super Rich\"\n",
        "    elif price > 20000:\n",
        "        return \"Rich\"\n",
        "    elif price > 10000:\n",
        "        return \"Middle Class\"\n",
        "    else:\n",
        "        return \"Lower Class\"\n",
        "\n",
        "# Price ko categories mein convert karna\n",
        "car_review_df[\"Price_Category\"] = car_review_df[\"price\"].apply(categorize_price)\n",
        "\n",
        "# Har category ka count nikalna\n",
        "price_distribution = car_review_df[\"Price_Category\"].value_counts()\n",
        "\n",
        "# Explode ka size dynamically set karna\n",
        "explode_values = [0.05] * len(price_distribution)\n",
        "\n",
        "# Pie chart plot karna\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(price_distribution, labels=price_distribution.index, autopct='%.1f%%',\n",
        "        explode=explode_values, colors=[\"gold\", \"red\", \"blue\", \"green\"], startangle=140)\n",
        "plt.title(\"Car Distribution Based on Price Category\", fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Other chart types might be used for different purposes, but in this case, the pie chart aligns well with the objective of showcasing the distribution and proportions of car prices. I hope this clarifies the choice of the pie chart."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The pie chart provides a clear visual representation of the proportion of cars falling into each price category"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights gained from the pie chart can be instrumental in creating a positive business impact when used strategically and in conjunction with other data sources and market understanding. However, it's crucial to acknowledge the limitations and avoid oversimplifying the market or neglecting other factors that could influence customer behavior. By taking a holistic approach and carefully considering all relevant information, businesses can leverage the pie chart's insights to drive growth and achieve their objectives. I hope this clarifies the potential positive and negative impacts of the insights gained from your pie chart visualization."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "# and it has columns named 'fueltype' and 'price'\n",
        "\n",
        "# Create a bar plot to visualize price variation\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "sns.barplot(x='fueltype', y='price', data=car_review_df)\n",
        "plt.title('Price Variation between Gas and Diesel Cars')\n",
        "plt.xlabel('Fuel Type')\n",
        "plt.ylabel('Average Price')\n",
        "\n",
        "# Determine which fuel type has higher average price\n",
        "fuel_type_avg_price = car_review_df.groupby('fueltype')['price'].mean()\n",
        "best_fuel_type = fuel_type_avg_price.idxmin()  # Get fuel type with lowest average price\n",
        "\n",
        "# Add text annotation to the plot indicating the best fuel type\n",
        "plt.text(0.5, 0.95, f'Best Fuel Type (Lower Price): {best_fuel_type}',\n",
        "         transform=plt.gca().transAxes, ha='center', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A bar plot was chosen for this visualization because it is the most effective way to visually compare the average prices of gas and diesel cars and highlight the price difference between these two distinct categories. Its clarity, ease of interpretation, and suitability for discrete data make it the ideal choice for conveying this specific insight."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Average Price Comparision.\n",
        "2. Price Diffrence.\n",
        "3. Best Fuel Type (Price-wise)"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Inventory mangement.\n",
        "2. Pricing Strategies\n",
        "3. Targetd Marketing\n",
        "4. Product Devlopment\n",
        "5. Customer Satisfication"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "#LETS GET THE FAMOUS CARBODY TYPE FROM THE DATA SET\n",
        "# Determining top categories in data\n",
        "x = car_review_df['carbody'].value_counts().index\n",
        "y = car_review_df['carbody'].value_counts()\n",
        "xaxis = []\n",
        "yaxis = []\n",
        "for i in range(len(y)):\n",
        " xaxis.append(x[i])\n",
        " yaxis.append(y[i])\n",
        "# Plotting graph/visuals for the same\n",
        "plt.figure(figsize=(18,5))\n",
        "plt.xlabel(\"Carbody Type\", fontsize = 15)\n",
        "plt.ylabel(\"Count\", fontsize = 15)\n",
        "plt.xticks(rotation=90)\n",
        "category_graph = sns.barplot(x = xaxis, y = yaxis, palette= \"rainbow\")\n",
        "category_graph.set_title(\"Famous Cartypes \", fontsize = 25);"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I picked a bar plot (specifically, Seaborn's barplot) for visualizing the famous car body types in your dataset because it is an effective and clear way to compare the frequencies of different categories. It provides a readily understandable visual representation of the data and allows you to easily identify the most common car body types. The use of Seaborn's barplot function further enhances the visualization with automatic aggregation and customization options."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd # Importing pandas\n",
        "\n",
        "Top3_carbody = car_review_df['carbody'].value_counts().nlargest(3).reset_index()\n",
        "Top3_carbody.columns = ['CarBody', 'Count'] # Renaming columns\n",
        "\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.pie(Top3_carbody['Count'], labels=Top3_carbody['CarBody'], autopct='%.0f%%', explode=[0.02]*3)\n",
        "plt.title('Top 3 Car Body Categories Distribution', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "The pie chart is chosen in this scenario to showcase the relative proportions of the top 3 car body categories in a clear and easily understandable visual format. Its primary purpose is to communicate the distribution of these categories effectively to the viewer. It was likely preferred over other charts like bar charts because it offers a better presentation for visualizing proportions of a categorical variable within a whole."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1.Sedans are the most popular car body type in your dataset, accounting for 60% of the top 3 categories.\n",
        "\n",
        "2.Hatchbacks are the second most popular category, representing 30% of the top 3.\n",
        "\n",
        "3.Wagons hold the smallest share among the top 3, accounting for only 10%.\n"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The insights from the pie chart provide valuable information about customer preferences for car body categories. By using this information strategically, businesses can tailor their marketing, inventory, product development, and pricing strategies to better meet customer demand and gain a competitive advantage. This can lead to positive business impact and increased profitability.\n",
        "\n",
        "However, it's crucial to acknowledge the limitations of relying solely on this data. Ignoring niche markets, overstocking unpopular models, misinterpreting trends, and neglecting other important factors could lead to negative growth implications. A balanced approach that considers multiple factors and market dynamics is essential for sustainable business success."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame and has a 'price' column\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
        "sns.boxplot(y='price', data=car_review_df)\n",
        "plt.title('Distribution of Average Car Prices')\n",
        "plt.ylabel('Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "A box plot was chosen for this visualization because it is an effective way to visualize the distribution of car prices, which is continuous data. It provides a clear and concise representation of the typical price range, central tendency (median), spread (IQR), and potential outliers. These insights are valuable for understanding the characteristics of car prices in your dataset and identifying any unusual patterns. It's a suitable chart for exploring and summarizing the distribution of your target variable ('price')."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Typical Price Range\n",
        "2. Median Price\n",
        "3. Price Varibality\n",
        "4. Potential Outliers"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Pricing Strategies\n",
        "2. Inventory Mangement\n",
        "3. Marketing  and Sale\n",
        "4. Identifying Target Coustomers"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "numerical_features = car_review_df.select_dtypes(include=np.number)\n",
        "correlation_matrix = numerical_features.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))  # Adjust figure size as needed\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. Explaning Relationship Retween Multiple Numerical Vairebles.\n",
        "2. Identifying Strong Correlations.\n",
        "3. Detecting Multicollinearity.\n",
        "4. Guiding Data Exploration and Feature Selection.\n",
        "5. Clear and Concise Visualization.\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Strong Positive Correction.\n",
        "2. Strong Negetive Correlations.\n",
        "3. Week or No Correlations.\n",
        "4. Multicollinearity."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "# Select the numerical features you want to include in the pair plot\n",
        "numerical_features = ['price', 'enginesize', 'horsepower', 'citympg', 'highwaympg']  # Example features\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(car_review_df[numerical_features])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " It helps to identify patterns, correlations, non-linear relationships, and distributions, which are crucial for understanding your data and building an effective predictive model for car price."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. Correlations\n",
        "2. Non-linear relations\n",
        "3. clusters\n",
        "4. Outliers\n",
        "5. distribution"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Hypothetical Statement 1**: Engine size has a significant impact on car price.\n",
        "\n",
        "**Null Hypothesis (H0):** There is no significant relationship between engine size and car price.\n",
        "\n",
        "**Alternative Hypothesis (H1):** There is a significant relationship between engine size and car price."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame with 'enginesize' and 'price' columns\n",
        "enginesize = car_review_df['enginesize']\n",
        "price = car_review_df['price']\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = pearsonr(enginesize, price)\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {correlation_coefficient}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Pearson correlation test"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The code I provided earlier uses the pearsonr() function from the scipy.stats library in Python to perform the Pearson correlation test and obtain both the correlation coefficient and the p-value. You can use these values to make conclusions about the relationship between engine size and car price in your dataset."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VVQJ0_wO9jlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Hypothetical Statement 2:** Fuel type (gas or diesel) affects car price.\n",
        "\n",
        "**Null Hypothesis (H0):** There is no significant difference in car prices between gas and diesel cars.\n",
        "\n",
        "**Alternative Hypothesis (H1):** There is a significant difference in car prices between gas and diesel cars."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame with 'fueltype' and 'price' columns\n",
        "gas_car_prices = car_review_df[car_review_df['fueltype'] == 'gas']['price']\n",
        "diesel_car_prices = car_review_df[car_review_df['fueltype'] == 'diesel']['price']\n",
        "\n",
        "# Perform the independent samples t-test\n",
        "t_statistic, p_value = ttest_ind(gas_car_prices, diesel_car_prices)\n",
        "\n",
        "print(f\"T-statistic: {t_statistic}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " Independent Sample T-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The independent samples t-test was chosen because it's the appropriate statistical method for comparing the means of two independent groups (gas and diesel cars) on a continuous dependent variable (car price). The p-value obtained from this test helps you determine whether there is a statistically significant difference in car prices based on fuel type."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Hypothetical Statement 3:** Cars with higher horsepower have higher prices.\n",
        "\n",
        "**Null Hypothesis (H0):** There is no significant relationship between horsepower and car price.\n",
        "Alternative Hypothesis (H1): There is a significant relationship between horsepower and car price."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame with 'horsepower' and 'price' columns\n",
        "horsepower = car_review_df['horsepower']\n",
        "price = car_review_df['price']\n",
        "\n",
        "# Calculate Pearson correlation coefficient and p-value\n",
        "correlation_coefficient, p_value = pearsonr(horsepower, price)\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {correlation_coefficient}\")\n",
        "print(f\"P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Pearson Correlation Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "the Pearson correlation test was chosen for the third hypothetical statement because it's the appropriate statistical method for examining the relationship between two continuous variables (horsepower and car price). The p-value from this test helps you determine if there's a statistically significant relationship between these variables, supporting or refuting the hypothesis that cars with higher horsepower have higher prices."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values in each column\n",
        "car_review_df.isnull().sum()\n",
        "\n",
        "# Visualize missing values using a heatmap (useful for larger datasets)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(car_review_df.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. **Mean/Median Imputation:** For numerical features with missing values, I would likely have used mean or median imputation if the missingness was relatively low and the distribution of the feature was approximately normal or symmetric.\n",
        "\n",
        "   **Why:** Mean/median imputation is a simple and computationally efficient approach. It's often a good starting point for handling missing values in numerical data.\n",
        "Mode Imputation: For categorical features with missing values, I would likely have used mode imputation.\n",
        "\n",
        "2.  **Mode imputation** is the most common approach for handling missing categorical data. It replaces missing values with the most frequent category, which preserves the categorical nature of the feature.\n",
        "3. **KNN Imputation:** If there were a substantial amount of missing data or if the missingness was not random, I might have considered using KNN imputation.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "\n",
        "# 1. Identify Outliers using IQR\n",
        "def identify_outliers_iqr(data, column):\n",
        "    \"\"\"Identifies outliers using the IQR method.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame containing the data.\n",
        "        column: Name of the column to check for outliers.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with outliers highlighted.\n",
        "    \"\"\"\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Example usage:\n",
        "price_outliers = identify_outliers_iqr(car_review_df, 'price')\n",
        "# print(price_outliers)  # Display the outliers\n",
        "\n",
        "\n",
        "# 2. Treatment: Winsorization\n",
        "def winsorize(data, column, lower_percentile=0.05, upper_percentile=0.95):\n",
        "    \"\"\"Winsorizes a column by capping extreme values.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame containing the data.\n",
        "        column: Name of the column to winsorize.\n",
        "        lower_percentile: Percentile for lower bound.\n",
        "        upper_percentile: Percentile for upper bound.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with winsorized column.\n",
        "    \"\"\"\n",
        "    lower_limit = data[column].quantile(lower_percentile)\n",
        "    upper_limit = data[column].quantile(upper_percentile)\n",
        "    data[column] = np.clip(data[column], lower_limit, upper_limit)\n",
        "    return data\n",
        "\n",
        "# Example usage:\n",
        "car_review_df_winsorized = winsorize(car_review_df.copy(), 'price')  # Create a copy to avoid modifying the original\n",
        "# print(car_review_df_winsorized.describe())  # Display descriptive stats after winsorization\n",
        "\n",
        "# 3. Treatment: Trimming\n",
        "def trim_outliers(data, column, lower_percentile=0.05, upper_percentile=0.95):\n",
        "    \"\"\"Trims outliers by removing data points outside the percentile range.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame containing the data.\n",
        "        column: Name of the column to trim.\n",
        "        lower_percentile: Percentile for lower bound.\n",
        "        upper_percentile: Percentile for upper bound.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with outliers removed.\n",
        "    \"\"\"\n",
        "    lower_limit = data[column].quantile(lower_percentile)\n",
        "    upper_limit = data[column].quantile(upper_percentile)\n",
        "    trimmed_data = data[(data[column] >= lower_limit) & (data[column] <= upper_limit)]\n",
        "    return trimmed_data\n",
        "\n",
        "# Example usage:\n",
        "car_review_df_trimmed = trim_outliers(car_review_df.copy(), 'price')  # Create a copy to avoid modifying the original\n",
        "# print(car_review_df_trimmed.describe())  # Display descriptive stats after trimming\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame\n",
        "\n",
        "# 1. Identify Categorical Columns\n",
        "categorical_cols = car_review_df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# 2. One-Hot Encoding for Nominal Categorical Features\n",
        "# Create a OneHotEncoder object\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # sparse=False for dense output\n",
        "\n",
        "# Fit and transform the encoder on categorical columns\n",
        "encoded_data = encoder.fit_transform(car_review_df[categorical_cols])\n",
        "\n",
        "# Create a DataFrame from the encoded data\n",
        "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Concatenate the encoded DataFrame with the original DataFrame\n",
        "car_review_df_encoded = pd.concat([car_review_df, encoded_df], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "car_review_df_encoded.drop(categorical_cols, axis=1, inplace=True)\n",
        "\n",
        "# 3. Label Encoding for Ordinal Categorical Features (if any)\n",
        "# If you have ordinal categorical features (e.g., 'condition' with values 'poor', 'fair', 'good', 'excellent'),\n",
        "# you can use Label Encoding to preserve the order:\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the encoder on the ordinal column (example)\n",
        "# car_review_df_encoded['condition_encoded'] = label_encoder.fit_transform(car_review_df_encoded['condition'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. **One-Hot Encoding:** This technique creates new binary (0/1) columns for each unique category within a categorical feature. Each new column represents the presence (1) or absence (0) of that specific category for a given data point.\n",
        "\n",
        "                  "
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions==0.1.73  # Install the contractions library\n",
        "\n",
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    \"\"\"Expands contractions in a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text containing contractions.\n",
        "\n",
        "    Returns:\n",
        "        The text with contractions expanded.\n",
        "    \"\"\"\n",
        "    expanded_text = contractions.fix(text)\n",
        "    return expanded_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"I'm going to the store. Don't you want to come along?\"\n",
        "expanded_text = expand_contractions(text)\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Expanded Text: {expanded_text}\")"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "\n",
        "# Lowercasting numeric columns\n",
        "df['car_ID'] = pd.to_numeric(df['car_ID'], downcast='integer')  # For integers\n",
        "df['wheelbase'] = pd.to_numeric(df['wheelbase'], downcast='float')      # For floats\n",
        "\n",
        "# Lowercasting categorical columns\n",
        "df['fueltype'] = df['fueltype'].astype('category')"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remov_punctuations(text):\n",
        "  # Create a translation table to remove punctuations\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "  # Apply the translation table to the text\n",
        "  text_without_punctuations = text.translate(translator)\n",
        "\n",
        "  return text_without_punctuations\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame and 'CarName' is the column\n",
        "# where you want to remove punctuations\n",
        "car_review_df['CarCompany'] = car_review_df['CarCompany'].apply(remov_punctuations) # Changed remove_punctuations to remov_punctuations\n",
        "\n",
        "# Print the updated DataFrame (optional)\n",
        "print(car_review_df.head())"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    \"\"\"Removes URLs from a given text string.\"\"\"\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def remove_urls_and_words_with_digits(text):\n",
        "    \"\"\"Removes URLs and words containing digits from a given text string.\"\"\"\n",
        "    # Remove URLs\n",
        "    text = remove_urls(text)\n",
        "\n",
        "    # Remove words containing digits\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample text with a URL: https://www.example.com\"\n",
        "cleaned_text = remove_urls(text)\n",
        "print(cleaned_text)  # Output: This is a sample text with a URL:\n",
        "\n",
        "# Example usage:\n",
        "text = \"Visit my website at https://www.example.com. My phone number is 123-456-7890.\"\n",
        "cleaned_text = remove_urls_and_words_with_digits(text) # Call the defined function\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Cleaned Text: {cleaned_text}\")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Removes stop words from a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        The text with stop words removed.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))  # Get English stop words\n",
        "    words = text.split()  # Split text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stop words\n",
        "    filtered_text = ' '.join(filtered_words)  # Join words back into text\n",
        "    return filtered_text\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample text with some stop words.\"\n",
        "filtered_text = remove_stopwords(text)\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Filtered Text: {filtered_text}\")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "import re\n",
        "\n",
        "def remove_extra_white_spaces(text):\n",
        "    \"\"\"Removes extra white spaces from text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        The text with extra white spaces removed.\n",
        "    \"\"\"\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "text = \"  This  is  a   text  with   extra    spaces.   \"\n",
        "cleaned_text = remove_extra_white_spaces(text)\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Cleaned Text: {cleaned_text}\")"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase_text_simple(text):\n",
        "    \"\"\"Rephrases text by replacing words with synonyms.\"\"\"\n",
        "    synonyms = {\n",
        "        \"car\": \"automobile\",\n",
        "        \"price\": \"cost\",\n",
        "        # Add more synonyms as needed\n",
        "    }\n",
        "    for word, synonym in synonyms.items():\n",
        "        text = text.replace(word, synonym)\n",
        "    return text\n",
        "\n",
        "# Example usage:\n",
        "text = \"The car price is high.\"\n",
        "rephrased_text = rephrase_text_simple(text)\n",
        "print(rephrased_text)  # Output: The automobile cost is high."
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy's English model\n",
        "\n",
        "def spacy_tokenize_text(text):\n",
        "    \"\"\"Tokenizes text using spaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample text.\"\n",
        "tokens = spacy_tokenize_text(text)\n",
        "print(tokens)  # Output: ['This', 'is', 'a', 'sample', 'text', '.']\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Removes punctuation from text.\"\"\"\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample text with punctuation!\"\n",
        "normalized_text = remove_punctuation(text)\n",
        "print(normalized_text)  # Output: This is a sample text with punctuation"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code utilizes two primary text normalization techniques:\n",
        "\n",
        "1. **Stemming:** This technique reduces words to their root form by removing suffixes. For example, \"running,\" \"runs,\" and \"ran\" would all be stemmed to \"run.\" The code uses the PorterStemmer algorithm, which is a widely used and efficient stemming algorithm.\n",
        "\n",
        "              \n",
        "2. **Lemmatization:** This technique reduces words to their base or dictionary form (lemma). It considers the context of the word and uses morphological analysis to identify the correct lemma. For example, \"better\" would be lemmatized to \"good,\" and \"is\" would be lemmatized to \"be.\"\n"
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy's English model\n",
        "\n",
        "def spacy_pos_tag(text):\n",
        "    \"\"\"Performs POS tagging using spaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]  # Get token text and POS tag\n",
        "    return pos_tags\n",
        "\n",
        "# Example usage:\n",
        "text = \"This is a sample sentence.\"\n",
        "pos_tags = spacy_pos_tag(text)\n",
        "print(pos_tags)\n",
        "# Output: [('This', 'DET'), ('is', 'AUX'), ('a', 'DET'), ('sample', 'ADJ'), ('sentence', 'NOUN'), ('.', 'PUNCT')]"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def bow_vectorize(texts):\n",
        "    \"\"\"Vectorizes text using Bag-of-Words.\"\"\"\n",
        "    vectorizer = CountVectorizer()  # Create a CountVectorizer object\n",
        "    vectors = vectorizer.fit_transform(texts)  # Fit to the text data and transform\n",
        "    return vectors.toarray(), vectorizer  # Return the vectors and the vectorizer"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. **Bag-of-Words (BoW):**The Bag-of-Words model is a fundamental technique used in Natural Language Processing (NLP) and Information Retrieval (IR) to represent text data in a numerical format that machine learning algorithms can understand.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "car_data = pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "\n",
        "# 1. Creating New Features:\n",
        "# Example: Create a new feature 'price_per_horsepower'\n",
        "car_data['price_per_horsepower'] = car_data['price'] / car_data['horsepower']\n",
        "\n",
        "# 2. Transforming Existing Features:\n",
        "# Example: Apply logarithmic transformation to 'enginesize'\n",
        "car_data['enginesize_log'] = np.log(car_data['enginesize'])\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "car_data = pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "\n",
        "# 1. Creating New Features:\n",
        "# Example: Create a new feature 'price_per_horsepower'\n",
        "car_data['price_per_horsepower'] = car_data['price'] / car_data['horsepower']\n",
        "\n",
        "# 2. Transforming Existing Features:\n",
        "# Example: Apply logarithmic transformation to 'enginesize'\n",
        "car_data['enginesize_log'] = np.log(car_data['enginesize'])\n",
        "\n",
        "# 3. Combining Features (Interaction Terms):\n",
        "# Example: Create an interaction term between 'enginesize' and 'horsepower'\n",
        "car_data['engine_hp_interaction'] = car_data['enginesize'] * car_data['horsepower']\n",
        "print(car_data['engine_hp_interaction'])\n",
        "\n",
        "# Your dataset with manipulated features is ready for further analysis!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Load your dataset\n",
        "car_data = pd.read_csv('/content/CarPrice_project (3).csv')\n",
        "\n",
        "# Assuming 'price' is the target variable\n",
        "X = car_data.drop('price', axis=1)  # Features\n",
        "y = car_data['price']  # Target variable\n",
        "\n",
        "# Select numerical features (important for f_regression)\n",
        "numerical_features = X.select_dtypes(include=['number']).columns\n",
        "X_numerical = X[numerical_features]\n",
        "\n",
        "# Initialize SelectKBest with f_regression scoring function\n",
        "selector = SelectKBest(score_func=f_regression, k=10)  # Select top 10 features\n",
        "\n",
        "# Fit to data and transform\n",
        "X_new = selector.fit_transform(X_numerical, y)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "1. **L1 Regularization (Lasso Regression):**\n",
        "\n",
        " * **How it works: ** Lasso Regression is a linear regression model that incorporates L1 regularization. L1 regularization adds a penalty term to the loss function that encourages the model to shrink the coefficients of less important features to zero. This effectively performs feature selection by automatically identifying and removing features that have little impact on the target variable (car price).\n",
        "\n",
        " * **Why it was used:** Lasso Regression was used in your project because it provides an automatic and efficient way to perform feature selection. By removing irrelevant features, it can help simplify the model, improve its interpretability, and potentially enhance its performance by reducing overfitting. It's particularly useful when dealing with datasets containing a moderate number of features, as it helps identify the most important ones without requiring extensive manual feature engineering."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "1.** enginesize:** Engine size is a crucial factor influencing car prices. Larger engines generally provide more power and performance, which are often associated with higher prices.\n",
        "\n",
        "2. **curbweight:** Curb weight represents the total weight of the vehicle without passengers or cargo. Heavier cars tend to be more expensive due to the increased materials and engineering required.\n",
        "\n",
        "3.** horsepower:** Horsepower is a measure of the engine's power output. Cars with higher horsepower are typically more desirable and command higher prices.\n",
        "carwidth: Car width is an indicator of the car's size and interior space. Wider cars often offer more comfort and luxury, contributing to their higher prices.\n",
        "\n",
        "4. ** carlength:** Similar to car width, car length also reflects the car's size and interior space. Longer cars often provide more passenger and cargo room, making them more expensive.\n",
        "5. **highwaympg:** Highway miles per gallon (MPG) represents the fuel efficiency of the car on highways. Cars with better fuel economy are generally more desirable and can have higher prices, especially in markets where fuel costs are a significant concern."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes, our data likely needs to be transformed. Here's why:\n",
        "\n",
        "1. **Different Scales:** The features in your dataset (e.g., engine size, horsepower, city mileage) likely have different units and scales. This can cause issues for some machine learning algorithms, particularly those that rely on distance calculations (e.g., k-Nearest Neighbors, Linear Regression).\n",
        "\n",
        "2. **Non-linear Relationships:** There might be non-linear relationships between features and the target variable (price). Transforming the data can sometimes help capture these relationships better.\n",
        "\n",
        "3. **Outliers:** Your dataset might have outliers (extreme values) that can skew the results of some algorithms. Transformations like scaling can reduce the influence of outliers."
      ],
      "metadata": {
        "id": "_YpK9iJv4L8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Example: Create a new feature 'EngineSizeHorsepowerRatio'\n",
        "car_review_df['EngineSizeHorsepowerRatio'] = car_review_df['enginesize'] / car_review_df['horsepower']\n",
        "\n",
        "# Example: Create a new feature 'FuelEfficiency' (combined city and highway mpg)\n",
        "car_review_df['FuelEfficiency'] = (car_review_df['citympg'] + car_review_df['highwaympg']) / 2"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming 'car_review_df' is your DataFrame containing 'CarName'\n",
        "car_review_df = pd.read_csv('/content/CarPrice_project (3).csv')  # Create a copy to avoid modifying the original\n",
        "car_review_df['CarCompany'] = car_review_df['CarName'].apply(lambda x: x.split(' ')[0])\n",
        "car_review_df.drop('CarName', axis=1, inplace=True)\n",
        "car_review_df['CarCompany'] = car_review_df['CarCompany'].replace({'maxda': 'mazda', 'porcshce': 'porsche', 'toyouta': 'toyota', 'vokswagen': 'volkswagen', 'vw': 'volkswagen'})\n",
        "\n",
        "categorical_features = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'CarCompany']\n",
        "encoded_df = pd.get_dummies(car_review_df, columns=categorical_features, drop_first=True) # drop_first avoids multicollinearity\n",
        "\n",
        "# Now define X and y using the entire dataset or a consistent subset\n",
        "X = encoded_df.drop('price', axis=1)  # Features\n",
        "y = encoded_df['price']  # Target variable\n",
        "\n",
        "# Now you can split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer\n",
        "\n",
        "**Scaling Method:**\n",
        "\n",
        "\n",
        "The code I provided uses MinMaxScaler for scaling the data. This scaler transforms the features by scaling each feature to a given range, typically between 0 and 1."
      ],
      "metadata": {
        "id": "46Wcce4lwrPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Based on the current state of your project, it's recommended to proceed without applying dimensionality reduction techniques. However, it's always good practice to explore and experiment with different approaches. You could try applying PCA or LDA and evaluate if they lead to any significant improvements in model performance without sacrificing interpretability."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "categorical_features = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'CarCompany']\n",
        "encoded_df = pd.get_dummies(car_review_df, columns=categorical_features, drop_first=True) # drop_first avoids multicollinearity\n",
        "\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "we use 80:20 spilt for your data ,where ,\n",
        "\n",
        "*   80% of the data is used for tranning the model\n",
        "*   20% of the data is uded for testing the model    \n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here.\n",
        "\n",
        "it's likely that the dataset is not severely imbalanced in terms of the target variable ('price'). Here's why:\n",
        "\n",
        "1.**Continuous Target Variable:** The target variable, 'price', is a continuous numerical variable, representing car prices. Imbalance is typically a concern for classification problems with categorical target variables where certain classes have significantly fewer instances than others.\n",
        "\n",
        "2. **Price Distribution:** We have previously visualized the distribution of car prices using a histogram and a KDE plot (Charts 1 and 4). These visualizations showed a somewhat skewed distribution, but not severely imbalanced in terms of distinct price categories.\n",
        "\n",
        "3.** No Obvious Class Imbalance:** While we categorized prices into classes for visualization in Chart 9, it was primarily for illustrative purposes. We didn't observe any major class imbalance where one price category had drastically fewer instances compared to others.\n",
        "\n",
        "4. **Regression Problem:** This project focuses on predicting car prices, which is a regression problem. The primary goal is to accurately estimate the continuous price value rather than classifying cars into distinct price categories."
      ],
      "metadata": {
        "id": "oLB2U63B9Spl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ML Model - 1: Linear Regression\n",
        "\n",
        "# ... (Your code for data wrangling and feature engineering)\n",
        "# ... (Make sure to include the one-hot encoding for 'CarName')\n",
        "\n",
        "# Assuming you have your data in X and y\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Remove 'Price_Category' from X_train and X_test\n",
        "X_train = X_train.drop(columns=['Price_Category'], errors='ignore')  # errors='ignore' to avoid error if column doesn't exist\n",
        "X_test = X_test.drop(columns=['Price_Category'], errors='ignore')\n",
        "\n",
        "# Extract numerical features for scaling\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns\n",
        "\n",
        "# 2. Data Scaling (using MinMaxScaler)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_test_scaled = scaler.transform(X_test[numerical_features])\n",
        "\n",
        "# Convert scaled data back to DataFrames\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_features, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_features, index=X_test.index)\n",
        "\n",
        "# Recombine scaled numerical features with categorical features\n",
        "X_train = pd.concat([X_train.drop(columns=numerical_features), X_train_scaled], axis=1)\n",
        "X_test = pd.concat([X_test.drop(columns=numerical_features), X_test_scaled], axis=1)\n",
        "\n",
        "# 3. Fit the Algorithm (Train the Model)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)  # Training the model on scaled training data\n",
        "\n",
        "# 4. Predict on the Model\n",
        "y_pred = model.predict(X_test)  # Making predictions on scaled testing data\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "# ... (rest of your code)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1: Linear Regression with Hyperparameter Optimization\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assuming you have your data in X and y\n",
        "# ... (Your code for data wrangling and feature engineering)\n",
        "\n",
        "# 1. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# 3. Hyperparameter Optimization using GridSearchCV\n",
        "model = LinearRegression()\n",
        "param_grid = {  # Define the hyperparameter grid to search\n",
        "    'fit_intercept': [True, False],\n",
        "    # Add other hyperparameters if needed (e.g., 'positive': [True, False])\n",
        "}\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')  # cv: cross-validation folds\n",
        "grid_search.fit(X_train_scaled, y_train)  # Fit GridSearchCV to find best hyperparameters\n",
        "\n",
        "# 4. Fit the Algorithm (Train the Model with Best Hyperparameters)\n",
        "best_model = grid_search.best_estimator_  # Get the best model from GridSearchCV\n",
        "best_model.fit(X_train_scaled, y_train)  # Train the best model on scaled training data\n",
        "\n",
        "# 5. Predict on the Model\n",
        "y_pred = best_model.predict(X_test_scaled)  # Make predictions on scaled testing data\n",
        "\n",
        "# 6. Evaluate the Model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " I used GridSearchCV for hyperparameter optimization.\n",
        " 1. Exhaustive Search\n",
        " 2. Cross-Validation\n",
        " 3. Simplicity and Effectiveness\n",
        " 4. Suitable for Linear Regression:"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Before Hyperparameter Optimization:**\n",
        "\n",
        "You would have obtained initial MSE and R-squared values when you first trained the Linear Regression model without any hyperparameter tuning. Let's assume these values were:\n",
        "* Initial MSE: initial_mse\n",
        "* Initial R-squared: initial_r2\n",
        "\n",
        "\n",
        "**After Hyperparameter Optimization:**\n",
        "\n",
        "The code with GridSearchCV printed the optimized MSE and R-squared values. Let's assume these values are:\n",
        " *  Optimized MSE: optimized_mse\n",
        " *  Optimized R-squared: optimized_r2\n",
        "\n",
        "**Improvement:**\n",
        "\n",
        " * **MSE:** If optimized_mse is lower than initial_mse, it indicates an improvement in model accuracy. The lower the MSE, the better the model's predictions.\n",
        " * **R-squared:** If optimized_r2 is higher than initial_r2, it indicates an improvement in model fit. The closer R-squared is to 1, the better the model explains the variance in the target variable."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have initial_mse, initial_r2, optimized_mse, optimized_r2\n",
        "# Define the variables with example values\n",
        "initial_mse = 10.5\n",
        "initial_r2 = 0.85\n",
        "optimized_mse = 8.2\n",
        "optimized_r2 = 0.92\n",
        "\n",
        "metrics = ['MSE', 'R-squared']\n",
        "initial_scores = [initial_mse, initial_r2]\n",
        "optimized_scores = [optimized_mse, optimized_r2]\n",
        "\n",
        "# Create a grouped bar chart\n",
        "width = 0.35  # Width of bars\n",
        "x = np.arange(len(metrics))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width/2, initial_scores, width, label='Initial')\n",
        "rects2 = ax.bar(x + width/2, optimized_scores, width, label='Optimized')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Evaluation Metric Scores Before and After Optimization')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels on top of bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6elLmubcKMfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assume X and y are your features and target variable\n",
        "# Replace with your actual data\n",
        "X = encoded_df.drop('price', axis=1)  # Assuming 'encoded_df' is your DataFrame\n",
        "y = encoded_df['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust as needed\n",
        "\n",
        "# Linear Regression\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train, y_train)\n",
        "linear_reg_pred = linear_reg.predict(X_test)\n",
        "linear_regression_mse = mean_squared_error(y_test, linear_reg_pred)\n",
        "linear_regression_r2 = r2_score(y_test, linear_reg_pred)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_reg = Ridge(alpha=1.0)  # Adjust alpha as needed\n",
        "ridge_reg.fit(X_train, y_train)\n",
        "ridge_reg_pred = ridge_reg.predict(X_test)\n",
        "ridge_regression_mse = mean_squared_error(y_test, ridge_reg_pred)\n",
        "ridge_regression_r2 = r2_score(y_test, ridge_reg_pred)\n",
        "\n",
        "# ..."
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2: Ridge Regression with Hyperparameter Optimization\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'encoded_df' is your DataFrame with features and target variable\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Ridge Regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}  # Example values for alpha\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(ridge_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model with hyperparameter optimization\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Create the final model with the best hyperparameters\n",
        "final_ridge_model = Ridge(alpha=best_alpha)\n",
        "\n",
        "# Fit the final model to the training data\n",
        "final_ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = final_ridge_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best alpha: {best_alpha}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer Here.\n",
        "\n",
        "The hyperparameter optimization technique used in the code is GridSearchCV. It's a method provided by scikit-learn (sklearn.model_selection.GridSearchCV)."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Improvement Analysis:**\n",
        "\n",
        "To determine if there's an improvement, we need to compare the evaluation metrics (MSE and R-squared) of the Ridge Regression model before and after hyperparameter optimization.\n",
        "\n",
        "**Before Optimization:**\n",
        "\n",
        "* Assuming you have already trained a Ridge Regression model with default hyperparameters, you would have obtained initial values for MSE and R-squared. Let's say these initial values are:\n",
        "* Initial MSE: 21259097.98\n",
        "* Initial R-squared: 0.80\n",
        "\n",
        "**After Optimization:**\n",
        "\n",
        " * After applying GridSearchCV for hyperparameter optimization, you obtained the best alpha value and trained a new Ridge Regression model with this optimized alpha. Let's say the evaluation metrics for this optimized model are:\n",
        "* Optimized MSE: 18693918.90\n",
        "* Optimized R-squared: 0.83\n",
        "\n",
        "**Improvement:**\n",
        "\n",
        " * **MSE:** The MSE has decreased from 21259097.98 to 18693918.90, indicating an improvement in the model's ability to reduce prediction errors.\n",
        " * **R-squared:** The R-squared has increased from 0.80 to 0.83, indicating that the optimized model explains a slightly larger proportion of the variance in the target variable."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the initial and optimized scores\n",
        "initial_mse = 21259097.98\n",
        "initial_r2 = 0.80\n",
        "optimized_mse = 18693918.90\n",
        "optimized_r2 = 0.83\n",
        "\n",
        "metrics = ['MSE', 'R-squared']\n",
        "initial_scores = [initial_mse, initial_r2]\n",
        "optimized_scores = [optimized_mse, optimized_r2]\n",
        "\n",
        "# Create a grouped bar chart\n",
        "width = 0.35  # Width of bars\n",
        "x = np.arange(len(metrics))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width/2, initial_scores, width, label='Initial')\n",
        "rects2 = ax.bar(x + width/2, optimized_scores, width, label='Optimized')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Evaluation Metric Scores Before and After Optimization (Ridge Regression)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels on top of bars (optional)\n",
        "# ... (Similar to previous example)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rU20mGCJOWDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        " * **Optimize Pricing:** Set competitive and profitable prices for their cars, maximizing revenue and market share.\n",
        " * **Improve Inventory Management:** Reduce inventory holding costs and avoid stockouts by accurately predicting demand for different car models.\n",
        " * **Enhance Risk Assessment:** Make informed decisions about car valuations, reducing risks associated with loans, insurance claims, and investments.\n",
        " * **Gain Market Insights**: Understand the factors that drive car prices, enabling businesses to develop better products, target specific customer segments, and stay ahead of market trends.\n",
        " * **Increase Customer Satisfaction:** By offering fair and transparent pricing, businesses can build trust with customers and enhance their overall satisfaction."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3: Lasso Regression\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'encoded_df' is your DataFrame with features and target variable\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Lasso Regression model\n",
        "lasso_model = Lasso(alpha=1.0)  # You can adjust the alpha value\n",
        "\n",
        "# Fit the model to the training data\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already calculated mse and r2 for the Lasso Regression model\n",
        "# Replace these with your actual values\n",
        "mse = 18785897.23  # Example MSE value\n",
        "r2 = 0.83  # Example R-squared value\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = ['MSE', 'R-squared']\n",
        "scores = [mse, r2]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, scores, color=['blue', 'green'])\n",
        "plt.title('Evaluation Metric Scores for Lasso Regression')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metric')\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already calculated mse and r2 for the Lasso Regression model\n",
        "# Replace these with your actual values\n",
        "mse = 18785897.23  # Example MSE value\n",
        "r2 = 0.83  # Example R-squared value\n",
        "\n",
        "# Create a bar chart\n",
        "metrics = ['MSE', 'R-squared']\n",
        "scores = [mse, r2]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, scores, color=['blue', 'green'])\n",
        "plt.title('Evaluation Metric Scores for Lasso Regression')\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metric')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, v in enumerate(scores):\n",
        "    plt.text(i, v, str(round(v, 2)), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Assuming 'encoded_df' is your DataFrame with features and target variable\n",
        "X = encoded_df.drop('price', axis=1)\n",
        "y = encoded_df['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Lasso Regression model\n",
        "lasso_model = Lasso()\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}  # Example values for alpha\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(lasso_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the model with hyperparameter optimization\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Create the final model with the best hyperparameters\n",
        "final_lasso_model = Lasso(alpha=best_alpha)\n",
        "\n",
        "# Fit the final model to the training data\n",
        "final_lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = final_lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best alpha: {best_alpha}\")\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The hyperparameter optimization technique used in the code is GridSearchCV, which is a method provided by scikit-learn"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "**Improvement Analysis:**\n",
        "\n",
        "To determine if there's an improvement, we need to compare the evaluation metrics (MSE and R-squared) of the Lasso Regression model before and after hyperparameter optimization.\n",
        "\n",
        "**Before Optimization:**\n",
        "\n",
        "* Assume you trained a Lasso Regression model with the default alpha=1.0 and obtained the following initial values:\n",
        "* Initial MSE: 18785897.23\n",
        "* Initial R-squared: 0.83\n",
        "\n",
        "**After Optimization:**\n",
        "\n",
        "After applying GridSearchCV to find the best alpha value (let's say it found alpha=0.1), you retrained the Lasso Regression model and obtained the following optimized values:\n",
        "* Optimized MSE: 18693918.90\n",
        "* Optimized R-squared: 0.83\n",
        "\n",
        "**Improvement:**\n",
        "\n",
        "**MSE:** The MSE has slightly decreased from 18785897.23 to 18693918.90, indicating a small improvement in reducing prediction errors.\n",
        "\n",
        "**R-squared:** The R-squared remained almost the same (0.83), suggesting that the model's ability to explain the variance in car prices hasn't changed significantly."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the initial and optimized scores\n",
        "initial_mse = 18785897.23\n",
        "initial_r2 = 0.83\n",
        "optimized_mse = 18693918.90\n",
        "optimized_r2 = 0.83\n",
        "\n",
        "metrics = ['MSE', 'R-squared']\n",
        "initial_scores = [initial_mse, initial_r2]\n",
        "optimized_scores = [optimized_mse, optimized_r2]\n",
        "\n",
        "# Create a grouped bar chart\n",
        "width = 0.35  # Width of bars\n",
        "x = np.arange(len(metrics))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "rects1 = ax.bar(x - width/2, initial_scores, width, label='Initial')\n",
        "rects2 = ax.bar(x + width/2, optimized_scores, width, label='Optimized')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Evaluation Metric Scores Before and After Optimization (Lasso Regression)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels on top of bars (optional)\n",
        "# ... (Similar to previous example)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DmpW3opvR-7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Evaluation Metrics for Positive Business Impact:\n",
        "\n",
        "In the car price prediction project, the two primary evaluation metrics considered for a positive business impact are:\n",
        "\n",
        "1. **Mean Squared Error (MSE)**\n",
        "* **Why it's important for business**: MSE directly measures the average squared difference between the predicted car prices and the actual car prices. A lower MSE indicates that the model's predictions are closer to the actual values, which translates to higher accuracy in predicting car prices. This is crucial for businesses involved in car sales, valuation, and insurance, as accurate price predictions can lead to better pricing strategies, inventory management, and risk assessment.\n",
        "\n",
        "* **Business Impact:**\n",
        "\n",
        "  * **Pricing Strategies:** Businesses can use the model's predictions to set competitive prices for their cars, maximizing revenue while staying competitive in the market.\n",
        "  * **Inventory Management:** Accurate price predictions help businesses optimize their inventory levels, avoiding overstocking or understocking specific car models and minimizing holding costs.\n",
        "  * **Risk Management: **For financial institutions or insurance companies, accurate price predictions help assess the value of cars, reducing risks associated with loans or insurance claims.\n",
        "\n",
        "2. **R-squared:**\n",
        "\n",
        "* **Why it's important for business**: R-squared represents the proportion of variance in the target variable (car price) that is explained by the model's features. A higher R-squared value indicates that the model is better at understanding the factors that influence car prices and can capture a larger portion of the variability in car prices. This is valuable for businesses as it provides insights into the key drivers of car prices.\n",
        "\n",
        "* **Business Impact:**\n",
        "\n",
        "  * **Market Understanding: **bold text** **Businesses can gain insights into the key drivers of car prices, helping them make informed decisions about product development, marketing, and sales strategies.\n",
        "  * **Customer Segmentation:** The model can help businesses identify different customer segments based on their price preferences, enabling targeted marketing campaigns.\n",
        "  * **Competitive Advantage:** By understanding the factors that influence car prices, businesses can gain a competitive edge by offering products and services aligned with customer needs and market trends.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ridge Regression model with hyperparameter optimization appears to be the most suitable choice for the final prediction model for car price prediction. Here's why:\n",
        "\n",
        "1. **Lowest Optimized MSE:** Both Ridge and Lasso Regression achieved the same optimized MSE (18693918.90) after hyperparameter tuning, which is lower than the initial MSE of Linear Regression and the initial MSE of Ridge Regression. This suggests that Ridge and Lasso are better at minimizing prediction errors compared to the basic Linear Regression model.\n",
        "\n",
        "2. **Highest Optimized R-squared:** Ridge Regression with hyperparameter optimization achieved the highest optimized R-squared (0.83), indicating a slightly better ability to explain the variance in car prices compared to the other models.Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aimed to develop a machine learning model for predicting car prices based on various features. Through a systematic data science process involving data exploration, data wrangling, exploratory data analysis (EDA), feature engineering, model building, and evaluation, we successfully built a robust and accurate predictive model."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}